import os
import streamlit as st
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from settings import LLM_MODEL

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìŠ¤ë¬´ê³ ê°œ AI ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì œëª©
st.title("ğŸ¤– ìŠ¤ë¬´ê³ ê°œ AI ì±—ë´‡")
st.markdown("---")


# OpenAI API í‚¤ í™•ì¸
@st.cache_resource
def initialize_model():
    """ëª¨ë¸ ì´ˆê¸°í™”"""
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    if not OPENAI_API_KEY:
        st.error("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        st.stop()

    model = ChatOpenAI(model=LLM_MODEL)

    template = """
            ë„ˆëŠ” ì´ì œë¶€í„° ë¬´ì¡°ê±´ ì„ì£¼ë‹˜ì„ ë¶™ì´ê³  ë§ëì—ëŠ” ë€¨~!ë¥¼ ë¶™ì—¬ ëŒ€ë‹µì„ í•´ì¤˜ì•¼í•´
            
            ì§ˆë¬¸:
            {question}
            """

    prompt = PromptTemplate.from_template(template)
    output_parser = StrOutputParser()
    chain = prompt | model | output_parser

    return chain


# ëª¨ë¸ ì´ˆê¸°í™”
chain = initialize_model()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“‹ ì„¤ì •")

    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”", type="secondary"):
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.markdown("### â„¹ï¸ ì •ë³´")
    st.markdown(f"- ëª¨ë¸: {LLM_MODEL}")

# ë©”ì¸ ì±„íŒ… ì˜ì—­
st.header("ğŸ’¬ ì±„íŒ…")

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            with st.spinner("ìƒê° ì¤‘..."):
                response_stream = chain.stream({"question": prompt})

                for chunk in response_stream:
                    full_response += chunk
                    message_placeholder.markdown(full_response + "â–Œ")

                # ë§ˆì§€ë§‰ì— ì»¤ì„œ ì œê±°
                message_placeholder.markdown(full_response)

        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            full_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ì„ì£¼ë‹˜. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            message_placeholder.markdown(full_response)

    # AI ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        Made with â¤ï¸ using Streamlit & LangChain
    </div>
    """,
    unsafe_allow_html=True
)
